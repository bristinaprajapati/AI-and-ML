{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpi5f-NuuRbg"
      },
      "source": [
        "##  **Some Helper Function:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDqrxMpLuhLO"
      },
      "source": [
        "### Softmax Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YoOjTJJpt6Nv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Compute the softmax probabilities for a given input matrix.\n",
        "\n",
        "    Parameters:\n",
        "    z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n",
        "                       - m is the number of samples.\n",
        "                       - n is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Softmax probability matrix of shape (m, n), where\n",
        "                   each row sums to 1 and represents the probability\n",
        "                   distribution over classes.\n",
        "\n",
        "    Notes:\n",
        "    - The input to softmax is typically computed as: z = XW + b.\n",
        "    - Uses numerical stabilization by subtracting the max value per row.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize input by subtracting the maximum value of each row from the corresponding row\n",
        "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    return exp_z/np.sum(exp_z, axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFnMdHJzrUJV"
      },
      "source": [
        "### Softmax Test Case:\n",
        "\n",
        "This test case checks that each row in the resulting softmax probabilities sums to 1, which is the fundamental property of softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL5ToHmkrTr-",
        "outputId": "7464d461-fe64-4b53-9bf5-2c48822fcede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax function passed the test case!\n"
          ]
        }
      ],
      "source": [
        "# Example test case\n",
        "z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n",
        "softmax_output = softmax(z_test)\n",
        "\n",
        "# Verify if the sum of probabilities for each row is 1 using assert\n",
        "row_sums = np.sum(softmax_output, axis=1)\n",
        "\n",
        "# Assert that the sum of each row is 1\n",
        "assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n",
        "\n",
        "print(\"Softmax function passed the test case!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1uPYyhotoAf"
      },
      "source": [
        "### Prediction Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8qwCbgC1vyHn"
      },
      "outputs": [],
      "source": [
        "def predict_softmax(X, W, b):\n",
        "    \"\"\"\n",
        "    Predict the class labels for a set of samples using the trained softmax model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c), where c is the number of classes.\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Predicted class labels of shape (n,), where each value is the index of the predicted class.\n",
        "    \"\"\"\n",
        "    z = np.dot(X,W) + b\n",
        "    y_pred = softmax(z)\n",
        "    predicted_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    return predicted_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCGDTavVuXZu"
      },
      "source": [
        "### Test Function for Prediction Function:\n",
        "The test function ensures that the predicted class labels have the same number of elements as the input samples, verifying that the model produces a valid output shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "musr99YhucQX",
        "outputId": "da071a94-cab5-4f6d-c08f-aa543ae92c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class labels: [1 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Define test case\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
        "\n",
        "# Expected Output:\n",
        "# The function should return an array with class labels (0, 1, or 2)\n",
        "\n",
        "y_pred_test = predict_softmax(X_test, W_test, b_test)\n",
        "\n",
        "# Validate output shape\n",
        "assert y_pred_test.shape == (3,), f\"Test failed: Expected shape (3,), got {y_pred_test.shape}\"\n",
        "\n",
        "# Print the predicted labels\n",
        "print(\"Predicted class labels:\", y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwejxbajvEle"
      },
      "source": [
        "### Loss Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bjqnULCtun_Z"
      },
      "outputs": [],
      "source": [
        "def loss_softmax(y_pred, y):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss for a single sample.\n",
        "\n",
        "    Parameters:\n",
        "    y_pred (numpy.ndarray): Predicted probabilities of shape (c,) for a single sample,\n",
        "                             where c is the number of classes.\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (c,), where c is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "    float: Cross-entropy loss for the given sample.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    loss = -np.sum(y * np.log(y_pred))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdMIV_cz5Fn"
      },
      "source": [
        "## Test case for Loss Function:\n",
        "This test case Compares loss for correct vs. incorrect predictions.\n",
        "*   Expects low loss for correct predictions.\n",
        "*   Expects high loss for incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IhRGquu0N9P",
        "outputId": "b6b038fc-d0b6-42b9-f241-0fd441323f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss (Correct Predictions): 0.4304\n",
            "Cross-Entropy Loss (Incorrect Predictions): 8.9872\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define correct predictions (low loss scenario)\n",
        "y_true_correct = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True one-hot labels\n",
        "y_pred_correct = np.array([[0.9, 0.05, 0.05],\n",
        "                           [0.1, 0.85, 0.05],\n",
        "                           [0.05, 0.1, 0.85]])  # High confidence in the correct class\n",
        "\n",
        "# Define incorrect predictions (high loss scenario)\n",
        "y_pred_incorrect = np.array([[0.05, 0.05, 0.9],  # Highly confident in the wrong class\n",
        "                              [0.1, 0.05, 0.85],\n",
        "                              [0.85, 0.1, 0.05]])\n",
        "\n",
        "# Compute loss for both cases\n",
        "loss_correct = loss_softmax(y_pred_correct, y_true_correct)\n",
        "loss_incorrect = loss_softmax(y_pred_incorrect, y_true_correct)\n",
        "\n",
        "# Validate that incorrect predictions lead to a higher loss\n",
        "assert loss_correct < loss_incorrect, f\"Test failed: Expected loss_correct < loss_incorrect, but got {loss_correct:.4f} >= {loss_incorrect:.4f}\"\n",
        "\n",
        "# Print results\n",
        "print(f\"Cross-Entropy Loss (Correct Predictions): {loss_correct:.4f}\")\n",
        "print(f\"Cross-Entropy Loss (Incorrect Predictions): {loss_incorrect:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0d3fm1-vUlY"
      },
      "source": [
        "### Cost Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yaH9_s0svIGJ"
      },
      "outputs": [],
      "source": [
        "def cost_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the average softmax regression cost (cross-entropy loss) over all samples.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where n is the number of samples and c is the number of classes.\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    float: Average softmax cost (cross-entropy loss) over all samples.\n",
        "    \"\"\"\n",
        "\n",
        "    n = X.shape[0]\n",
        "    total_loss = 0\n",
        "    for i in range(n):\n",
        "        x_i = X[i::]\n",
        "        y_i = y[i::]\n",
        "        z = np.dot(x_i,W) + b\n",
        "        y_pred = softmax(z)\n",
        "        loss = loss_softmax(y_pred, y_i)\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eGyPFJ33tgY"
      },
      "source": [
        "### Test Case for Cost Function:\n",
        "The test case assures that the cost for the incorrect prediction should be higher than for the correct prediction, confirming that the cost function behaves as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIGAxYQt36Sr",
        "outputId": "226bcce6-6bc1-4509-9bb5-40f9bc263faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost for correct prediction: 0.0007911395997827637\n",
            "Cost for incorrect prediction: 0.4115114121180463\n",
            "Test passed!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example 1: Correct Prediction (Closer predictions)\n",
        "X_correct = np.array([[1.0, 0.0], [0.0, 1.0]])  # Feature matrix for correct predictions\n",
        "y_correct = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, matching predictions)\n",
        "W_correct = np.array([[5.0, -2.0], [-3.0, 5.0]])  # Weights for correct prediction\n",
        "b_correct = np.array([0.1, 0.1])  # Bias for correct prediction\n",
        "\n",
        "# Example 2: Incorrect Prediction (Far off predictions)\n",
        "X_incorrect = np.array([[0.1, 0.9], [0.8, 0.2]])  # Feature matrix for incorrect predictions\n",
        "y_incorrect = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, incorrect predictions)\n",
        "W_incorrect = np.array([[0.1, 2.0], [1.5, 0.3]])  # Weights for incorrect prediction\n",
        "b_incorrect = np.array([0.5, 0.6])  # Bias for incorrect prediction\n",
        "\n",
        "# Compute cost for correct predictions\n",
        "cost_correct = cost_softmax(X_correct, y_correct, W_correct, b_correct)\n",
        "\n",
        "# Compute cost for incorrect predictions\n",
        "cost_incorrect = cost_softmax(X_incorrect, y_incorrect, W_incorrect, b_incorrect)\n",
        "\n",
        "# Check if the cost for incorrect predictions is greater than for correct predictions\n",
        "assert cost_incorrect > cost_correct, f\"Test failed: Incorrect cost {cost_incorrect} is not greater than correct cost {cost_correct}\"\n",
        "\n",
        "# Print the costs for verification\n",
        "print(\"Cost for correct prediction:\", cost_correct)\n",
        "print(\"Cost for incorrect prediction:\", cost_incorrect)\n",
        "\n",
        "print(\"Test passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-YIb7zlveKq"
      },
      "source": [
        "### Computing Gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G3Vpn5bNvW3x"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the gradients of the cost function with respect to weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "    tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "\n",
        "    grad_W = np.dot(X.T, (y_pred - y)) / n  # Gradient with respect to weights\n",
        "    grad_b = np.sum(y_pred - y, axis=0) / n  # Gradient with respect to biases\n",
        "\n",
        "\n",
        "    return grad_W, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84yoIUx7vY7"
      },
      "source": [
        "### Test case for compute_gradient function:\n",
        "The test checks if the gradients from the function are close enough to the manually computed gradients using np.allclose, which accounts for potential floating-point discrepancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-YSC_Ot70bZ",
        "outputId": "db5b2f1b-1ada-42fe-ea5c-96b9351877a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient w.r.t. W: [[ 0.1031051   0.01805685 -0.12116196]\n",
            " [-0.13600547  0.00679023  0.12921524]]\n",
            "Gradient w.r.t. b: [-0.03290036  0.02484708  0.00805328]\n",
            "Test passed!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple feature matrix and true labels\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
        "y_test = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True labels (one-hot encoded, 3 classes)\n",
        "\n",
        "# Define weight matrix and bias vector\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
        "\n",
        "# Compute the gradients using the function\n",
        "grad_W, grad_b = compute_gradient_softmax(X_test, y_test, W_test, b_test)\n",
        "\n",
        "# Manually compute the predicted probabilities (using softmax function)\n",
        "z_test = np.dot(X_test, W_test) + b_test\n",
        "y_pred_test = softmax(z_test)\n",
        "\n",
        "# Compute the manually computed gradients\n",
        "grad_W_manual = np.dot(X_test.T, (y_pred_test - y_test)) / X_test.shape[0]\n",
        "grad_b_manual = np.sum(y_pred_test - y_test, axis=0) / X_test.shape[0]\n",
        "\n",
        "# Assert that the gradients computed by the function match the manually computed gradients\n",
        "assert np.allclose(grad_W, grad_W_manual), f\"Test failed: Gradients w.r.t. W are not equal.\\nExpected: {grad_W_manual}\\nGot: {grad_W}\"\n",
        "assert np.allclose(grad_b, grad_b_manual), f\"Test failed: Gradients w.r.t. b are not equal.\\nExpected: {grad_b_manual}\\nGot: {grad_b}\"\n",
        "\n",
        "# Print the gradients for verification\n",
        "print(\"Gradient w.r.t. W:\", grad_W)\n",
        "print(\"Gradient w.r.t. b:\", grad_b)\n",
        "\n",
        "print(\"Test passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W75VL71ivpjG"
      },
      "source": [
        "### Implementing Gradient Descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bbQ7SVw7vo-M"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "    b (numpy.ndarray): Bias vector of shape (c,).\n",
        "    alpha (float): Learning rate.\n",
        "    n_iter (int): Number of iterations.\n",
        "    show_cost (bool): Whether to display the cost at intervals.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Optimized weights, biases, and cost history.\n",
        "    \"\"\"\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "        W -= alpha * grad_W\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        cost = cost_softmax(X, y, W, b)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Print cost at regular intervals\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "\n",
        "    return W, b, cost_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBG9uSWKHDgX"
      },
      "source": [
        "## Preparing Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "prZ_zAvLpodE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_and_prepare_mnist(csv_file, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Reads the MNIST CSV file, splits data into train/test sets, and plots one image per class.\n",
        "\n",
        "    Arguments:\n",
        "    csv_file (str)       : Path to the CSV file containing MNIST data.\n",
        "    test_size (float)    : Proportion of the data to use as the test set (default: 0.2).\n",
        "    random_state (int)   : Random seed for reproducibility (default: 42).\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_test, y_train, y_test : Split dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Separate labels and features\n",
        "    y = df.iloc[:, 0].values  # First column is the label\n",
        "    X = df.iloc[:, 1:].values  # Remaining columns are pixel values\n",
        "\n",
        "    # Normalize pixel values (optional but recommended)\n",
        "    X = X / 255.0  # Scale values between 0 and 1\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Plot one sample image per class\n",
        "    plot_sample_images(X, y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def plot_sample_images(X, y):\n",
        "    \"\"\"\n",
        "    Plots one sample image for each digit class (0-9).\n",
        "\n",
        "    Arguments:\n",
        "    X (np.ndarray): Feature matrix containing pixel values.\n",
        "    y (np.ndarray): Labels corresponding to images.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    unique_classes = np.unique(y)  # Get unique class labels\n",
        "\n",
        "    for i, digit in enumerate(unique_classes):\n",
        "        index = np.where(y == digit)[0][0]  # Find first occurrence of the class\n",
        "        image = X[index].reshape(28, 28)  # Reshape 1D array to 28x28\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.title(f\"Digit: {digit}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAyQau8iHOqo",
        "outputId": "99fe7021-d98d-4f25-9404-9f42c048f4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "ZtYR42Qas2uf",
        "outputId": "d32a0442-296b-4053-d324-2beae0caf8a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "csv_file_path = \"/content/drive/MyDrive/Colab Notebooks/Artificial Intelligence and Machine Learning/mnist_test.csv\"  # Path to saved dataset\n",
        "X_train, X_test, y_train, y_test = load_and_prepare_mnist(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMBH4mQtzHA"
      },
      "source": [
        "### **A Quick debugging Step:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIJhtnuCs7QF",
        "outputId": "6e0651b4-f983-46a8-8f65-1528a085ed50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Move forward: Dimension of Feture Matrix X and label vector y matched.\n"
          ]
        }
      ],
      "source": [
        "# Assert that X and y have matching lengths\n",
        "assert len(X_train) == len(y_train), f\"Error: X and y have different lengths! X={len(X_train)}, y={len(y_train)}\"\n",
        "print(\"Move forward: Dimension of Feture Matrix X and label vector y matched.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TKIsKJcwFsv"
      },
      "source": [
        "## **Train the Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuTbCU0xAQW",
        "outputId": "a3e70310-440f-4fcc-af89-6539db23d6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (7999, 784)\n",
            "Test data shape: (2000, 784)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8e2mHmRv4fd",
        "outputId": "b27138e7-5b41-41a1-ab8e-68acfc2c9d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 8790.973616\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Check if y_train is one-hot encoded\n",
        "if len(y_train.shape) == 1:\n",
        "    encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output=False for newer versions of sklearn\n",
        "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))  # One-hot encode labels\n",
        "    y_test = encoder.transform(y_test.reshape(-1, 1))  # One-hot encode test labels\n",
        "\n",
        "# Now y_train is one-hot encoded, and we can proceed to use it\n",
        "d = X_train.shape[1]  # Number of features (columns in X_train)\n",
        "c = y_train.shape[1]  # Number of classes (columns in y_train after one-hot encoding)\n",
        "\n",
        "# Initialize weights with small random values and biases with zeros\n",
        "W = np.random.randn(d, c) * 0.01  # Small random weights initialized\n",
        "b = np.zeros(c)  # Bias initialized to 0\n",
        "\n",
        "# Set hyperparameters for gradient descent\n",
        "alpha = 0.1  # Learning rate\n",
        "n_iter = 1000  # Number of iterations to run gradient descent\n",
        "\n",
        "# Train the model using gradient descent\n",
        "W_opt, b_opt, cost_history = gradient_descent_softmax(X_train, y_train, W, b, alpha, n_iter, show_cost=True)\n",
        "\n",
        "# Plot the cost history to visualize the convergence\n",
        "plt.plot(cost_history)\n",
        "plt.title('Cost Function vs. Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH4wNbhzys4f"
      },
      "source": [
        "## **Evaluating the Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzV7BkRqOl5A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate classification performance using confusion matrix, precision, recall, and F1-score.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (numpy.ndarray): True labels\n",
        "    y_pred (numpy.ndarray): Predicted labels\n",
        "\n",
        "    Returns:\n",
        "    tuple: Confusion matrix, precision, recall, F1 score\n",
        "    \"\"\"\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Compute precision, recall, and F1-score\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    return cm, precision, recall, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuGtvIlywK7J"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "y_pred_test = predict_softmax(X_test, W_opt, b_opt)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # True labels in numeric form\n",
        "\n",
        "# Evaluate the model\n",
        "cm, precision, recall, f1 = evaluate_classification(y_test_labels, y_pred_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Visualizing the Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "cax = ax.imshow(cm, cmap='Blues')  # Use a color map for better visualization\n",
        "\n",
        "# Dynamic number of classes\n",
        "num_classes = cm.shape[0]\n",
        "ax.set_xticks(range(num_classes))\n",
        "ax.set_yticks(range(num_classes))\n",
        "ax.set_xticklabels([f'Predicted {i}' for i in range(num_classes)])\n",
        "ax.set_yticklabels([f'Actual {i}' for i in range(num_classes)])\n",
        "\n",
        "# Add labels to each cell in the confusion matrix\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i, j] > np.max(cm) / 2 else 'black')\n",
        "\n",
        "# Add grid lines and axis labels\n",
        "ax.grid(False)\n",
        "plt.title('Confusion Matrix', fontsize=14)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('Actual Label', fontsize=12)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.colorbar(cax)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv8J5hNCPzl6"
      },
      "source": [
        "# Linear Seperability and Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTyaPcM-P69S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}